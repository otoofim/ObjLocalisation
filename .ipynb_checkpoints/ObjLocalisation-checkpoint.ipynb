{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.23) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/home/mohammad/Tensorflow/local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./lib/\")\n",
    "\n",
    "import VOC2012DataProvider\n",
    "import VOC2012_npz_files_writter\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from Agent import ObjLocaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records are already prepared!!!\n"
     ]
    }
   ],
   "source": [
    "#This cell reads VOC 2012 dataset and save them in .npz files for future.\n",
    "#The process of reading data and put them in prper format is time consuming so they are stored in a file.\n",
    "\n",
    "xml_path = \"../VOC2012/Annotations/*.xml\"\n",
    "destination = \"../data/\"\n",
    "\n",
    "#It splits dataset to 80% for training and 20% validation.\n",
    "if not (os.path.isfile(destination+\"test_input.npz\") or os.path.isfile(destination+\"test_target.npz\")):\n",
    "    VOC2012_npz_files_writter.writting_files(xml_path, destination, percentage=0)\n",
    "    print(\"Files are ready!!!\")\n",
    "else:\n",
    "    print(\"Records are already prepared!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = VOC2012DataProvider.PascalDataProvider('train', batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil = Image.frombytes(\"RGB\",(inputs.inputs[0]['image_width'],inputs.inputs[0]['image_height']),inputs.inputs[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 500, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pil).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.# Convo \n",
    "filter_size1 = 7          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 96         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 7          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 265        # There are 36 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size2 = 3          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 384        # There are 36 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 3          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 384        # There are 36 of these filters.\n",
    "\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc1_size = 4096           # Number of neurons in fully-connected layer.\n",
    "fc2_size = 1024           # Number of neurons in fully-connected layer.\n",
    "fc3_size = 1024           # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 224\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 3\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_actions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05),name='W')\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]),name='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   filter_stride,\n",
    "                   use_pooling,   # Use max-pooling.\n",
    "                   pooling_size = None,\n",
    "                   pooling_stride = None,\n",
    "                   name = 'Conv'):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "\n",
    "        # Shape of the filter-weights for the convolution.\n",
    "        # This format is determined by the TensorFlow API.\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "        # Create new weights aka. filters with the given shape.\n",
    "        weights = new_weights(shape=shape)\n",
    "\n",
    "        # Create new biases, one for each filter.\n",
    "        biases = new_biases(length=num_filters)\n",
    "\n",
    "        # Create the TensorFlow operation for convolution.\n",
    "        # Note the strides are set to 1 in all dimensions.\n",
    "        # The first and last stride must always be 1,\n",
    "        # because the first is for the image-number and\n",
    "        # the last is for the input-channel.\n",
    "        # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "        # is moved 2 pixels across the x- and y-axis of the image.\n",
    "        # The padding is set to 'SAME' which means the input image\n",
    "        # is padded with zeroes so the size of the output is the same.\n",
    "        layer = tf.nn.conv2d(input=input,\n",
    "                             filter=weights,\n",
    "                             strides=[1, filter_stride, filter_stride, 1], #strides=[batch, height, width, channels]\n",
    "                             padding='SAME') # 'SAME' means zero-padding\n",
    "\n",
    "        # Add the biases to the results of the convolution.\n",
    "        # A bias-value is added to each filter-channel.\n",
    "        layer += biases\n",
    "\n",
    "        # Use pooling to down-sample the image resolution?\n",
    "        if use_pooling:\n",
    "            # This is 2x2 max-pooling, which means that we\n",
    "            # consider 2x2 windows and select the largest value\n",
    "            # in each window. Then we move 2 pixels to the next window.\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, pooling_size, pooling_size, 1],\n",
    "                                   strides=[1, pooling_stride, pooling_stride, 1], #strides=[batch, height, width, channels]\n",
    "                                   padding='SAME') # 'SAME' means zero-padding\n",
    "\n",
    "        # Rectified Linear Unit (ReLU).\n",
    "        # It calculates max(x, 0) for each input pixel x.\n",
    "        # This adds some non-linearity to the formula and allows us\n",
    "        # to learn more complicated functions.\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", weights)\n",
    "        tf.summary.histogram(\"biases\", biases)\n",
    "        tf.summary.histogram(\"activations\", layer)\n",
    "\n",
    "        # Note that ReLU is normally executed before the pooling,\n",
    "        # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "        # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "        # We return both the resulting layer and the filter-weights\n",
    "        # because we will plot the weights later.\n",
    "        return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,\n",
    "                 name = 'fc'): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        # Create new weights and biases.\n",
    "        weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "        biases = new_biases(length=num_outputs)\n",
    "\n",
    "        # Calculate the layer as the matrix multiplication of\n",
    "        # the input and weights, and then add the bias-values.\n",
    "        layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        # Use ReLU?\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "            \n",
    "        tf.summary.histogram(\"weights\", weights)\n",
    "        tf.summary.histogram(\"biases\", biases)\n",
    "        tf.summary.histogram(\"activations\", layer)\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "tf.summary.image('input', x_image, 1)\n",
    "#y_true = tf.placeholder(tf.float32, shape=[None, num_actions], name='y_true')\n",
    "#y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=3,\n",
    "                   filter_size=7,\n",
    "                   num_filters=96,\n",
    "                   filter_stride = 2,\n",
    "                   use_pooling=True,   # Use max-pooling.\n",
    "                   pooling_size = 3,\n",
    "                   pooling_stride = 2,\n",
    "                   name = 'conv1')\n",
    "\n",
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=96,\n",
    "                   filter_size=5,\n",
    "                   num_filters=256,\n",
    "                   filter_stride = 2,\n",
    "                   use_pooling=True,   # Use max-pooling.\n",
    "                   pooling_size = 3,\n",
    "                   pooling_stride = 1,\n",
    "                   name = 'conv2')\n",
    "\n",
    "layer_conv3, weights_conv3 = \\\n",
    "    new_conv_layer(input=layer_conv2,\n",
    "                   num_input_channels=256,\n",
    "                   filter_size=3,\n",
    "                   num_filters=384,\n",
    "                   filter_stride = 1,\n",
    "                   use_pooling=False,   # Use max-pooling.\n",
    "                   name = 'conv3')\n",
    "\n",
    "layer_conv4, weights_conv4 = \\\n",
    "    new_conv_layer(input=layer_conv3,\n",
    "                   num_input_channels=384,\n",
    "                   filter_size=3,\n",
    "                   num_filters=384,\n",
    "                   filter_stride = 1,\n",
    "                   use_pooling=False,  \n",
    "                   name = 'conv4')\n",
    "\n",
    "layer_conv5, weights_conv5 = \\\n",
    "    new_conv_layer(input=layer_conv4,\n",
    "                   num_input_channels=384,\n",
    "                   filter_size=0,\n",
    "                   num_filters=0,\n",
    "                   filter_stride = 1, #it should be 0 but becuase number of filter is 0 there shouldn't be any problem\n",
    "                   use_pooling=True,   # Use max-pooling.\n",
    "                   pooling_size = 3,\n",
    "                   pooling_stride = 2,\n",
    "                   name = 'conv5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv5)\n",
    "\n",
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=1024,\n",
    "                         use_relu=True,\n",
    "                         name = 'fc1')\n",
    "\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=1024,\n",
    "                         num_outputs=1024,\n",
    "                         use_relu=False,\n",
    "                         name = 'fc2')\n",
    "\n",
    "layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                         num_inputs=1024,\n",
    "                         num_outputs=num_actions,\n",
    "                         use_relu=False,\n",
    "                         name = 'fc3' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qout = tf.nn.softmax(layer_fc3)\n",
    "#y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,labels=y_true)\n",
    "#cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,num_actions],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizeroptimize  = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "#correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d6600b2adf33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mfirst_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtarget_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xmin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ymax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ymin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mPILimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_width'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_height'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdqn_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObjLocaliser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPILimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in inputs:\n",
    "    for i in range(len(input_batch)):\n",
    "        first_obj = [target_batch[i]['xmax'][0], target_batch[i]['xmin'][0], target_batch[i]['ymax'][0], target_batch[i]['ymin'][0]]\n",
    "        PILimage = Image.frombytes(\"RGB\",(inputs.inputs[i]['image_width'],inputs.inputs[i]['image_height']),inputs.inputs[i]['image'])\n",
    "        dqn_agent = ObjLocaliser(PILimage, first_obj)\n",
    "        #break\n",
    "        for _ in range(50):\n",
    "            \n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={x: input_batch[i]['image']})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "            jList.append(j)\n",
    "            rList.append(rAll)\n",
    "            \n",
    "            \n",
    "            #_, batch_error, batch_acc = sess.run(\n",
    "            #    [optimizer, cost, accuracy], \n",
    "            #    feed_dict={x: input_batch[i]['image']})\n",
    "        print(input_batch[i]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=dqn_agent.wrapping()\n",
    "b=np.array(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(10):\n",
    "    \n",
    "    running_error = 0.\n",
    "    running_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in train_data:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        _, batch_error, batch_acc = sess.run(\n",
    "            [optimizer, cost, accuracy], \n",
    "            feed_dict={x: input_batch, y_true: target_batch})\n",
    "        running_error += batch_error\n",
    "        running_accuracy += batch_acc\n",
    "    running_error /= train_data.num_batches\n",
    "    running_accuracy /= train_data.num_batches\n",
    "    print('End of epoch {0:02d}: err(train)={1:.2f} acc(train)={2:.2f}'\n",
    "          .format(e + 1, running_error, running_accuracy))\n",
    "    if (e + 1) % 5 == 0:\n",
    "        valid_error = 0.\n",
    "        valid_accuracy = 0.\n",
    "        for input_batch, target_batch in valid_data:\n",
    "            batch_error, batch_acc = sess.run([cost, accuracy],feed_dict={x: input_batch, y_true: target_batch})\n",
    "            valid_error += batch_error\n",
    "            valid_accuracy += batch_acc\n",
    "        valid_error /= valid_data.num_batches\n",
    "        valid_accuracy /= valid_data.num_batches\n",
    "        print('                 err(valid)={0:.2f} acc(valid)={1:.2f}'\n",
    "               .format(valid_error, valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to load data from pascal VOC: https://gist.github.com/saghiralfasly/ee642af0616461145a9a82d7317fb1d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing IoU: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "https://angusg.com/writing/2016/12/28/optimizing-iou-semantic-segmentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
